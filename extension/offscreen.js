function debugLog(...args) {
  console.log(...args);
  const serializedArgs = args.map(arg => {
    if (arg instanceof MediaStream) return `[MediaStream: ${arg.id}, active: ${arg.active}]`;
    if (arg instanceof AudioContext) return `[AudioContext: ${arg.state}]`;
    if (arg instanceof MediaStreamAudioSourceNode) return `[MediaStreamAudioSourceNode]`;
    return arg;
  });
  chrome.runtime.sendMessage({ type: "LOG", data: serializedArgs });
}

debugLog("Offscreen script loaded");
chrome.runtime.sendMessage({ type: "OFFSCREEN_LOADED" });

let mediaStream;
let pc;
let signaling;

chrome.runtime.onMessage.addListener(async (message, sender, sendResponse) => {
  debugLog("Received message:", message.type);

  if (message.type === "STOP_CAPTURE") {
    if (mediaStream) {
      mediaStream.getTracks().forEach(track => track.stop());
      mediaStream = null;
    }
    if (pc) {
      pc.close();
      pc = null;
    }
    if (signaling) {
      signaling.close();
      signaling = null;
    }
    sendResponse("CAPTURE_STOPPED");
    return;
  }

  if (message.type !== "START_CAPTURE") return;

  if (mediaStream) {
    mediaStream.getTracks().forEach(track => track.stop());
    mediaStream = null;
  }

  try {
    mediaStream = await navigator.mediaDevices.getUserMedia({
      audio: {
        mandatory: {
          chromeMediaSource: "tab",
          chromeMediaSourceId: message.streamId,
        },
      },
      video: false,
    });

    debugLog("Stream captured:", mediaStream);

    // Audio Context to keep the stream alive (Chrome kills silent streams)
    const audioContext = new AudioContext();
    const source = audioContext.createMediaStreamSource(mediaStream);
    source.connect(audioContext.destination);

    setupWebRTC(mediaStream);
  } catch (err) {
    debugLog("Error starting capture:", err);
  }
});

function setupWebRTC(stream) {
  debugLog("Setting up WebRTC connection...");
  
  // Connect to the desktop signaling server
  signaling = new WebSocket("ws://localhost:8080");

  pc = new RTCPeerConnection({
    iceServers: [{ urls: "stun:stun.l.google.com:19302" }],
  });

  // Add audio tracks to the peer connection
  stream.getTracks().forEach(track => pc.addTrack(track, stream));

  // Handle ICE candidates generated by the browser
  pc.onicecandidate = event => {
    if (event.candidate) {
      signaling.send(JSON.stringify({ type: "candidate", candidate: event.candidate }));
    }
  };

  pc.onconnectionstatechange = () => {
    debugLog("PeerConnection State:", pc.connectionState);
  };

  signaling.onopen = async () => {
    debugLog("Connected to signaling server. Creating offer...");
    try {
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);
      signaling.send(JSON.stringify({ type: "offer", offer }));
    } catch (e) {
      debugLog("Error creating offer:", e);
    }
  };

  signaling.onmessage = async (event) => {
    try {
      const data = JSON.parse(event.data);
      if (data.type === "answer") {
        debugLog("Received answer from desktop");
        await pc.setRemoteDescription(new RTCSessionDescription(data.answer));
      } else if (data.type === "candidate") {
        debugLog("Received ICE candidate from desktop");
        await pc.addIceCandidate(new RTCIceCandidate(data.candidate));
      }
    } catch (e) {
      debugLog("Error handling signaling message:", e);
    }
  };

  signaling.onerror = (error) => {
    debugLog("Signaling WebSocket Error:", error);
  };
}